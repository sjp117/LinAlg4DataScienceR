---
title: "Chapter 9: Orthogonal Matrices and QR Decomposition"
output: html_document
date: "`r Sys.Date()`"
---

```{r}
library(ggplot2)
library(tidyr)
```


## Orthogonal Matrices

Orthogonal Columns:

\- All columns are pair-wise orthogonal.

Unit-norm columns:

\- The norm (geometric length) of each column is 1.

```{python}
import numpy as np

Q1 = np.array([ [1,-1],[1,1] ]) / np.sqrt(2)
Q2 = np.array([ [1,2,2],[2,1,-2],[-2,2,-1] ]) / 3

print( Q1.T @ Q1 )
print( Q2.T @ Q2 )
```

## Gram-Schmidt

```{r}
paradecomp <- function(r,t) {
    return( r %*% ((t(t) %*% r) / (t(r) %*% r)) )
}

perpendecomp <- function(r,t) {
    return( t - paradecomp(r,t) )
}
```

```{r}
mat1 <- matrix(data = rnorm(16), nrow = 4)

for (j in 1:ncol(mat1)) {
    
}
```

## QR Decomposition

```{r}
A <- matrix(data = rnorm(6*6), nrow = 6)
Q <- qr(A) |> qr.Q()
R <- qr(A) |> qr.R()

plotly::plot_ly(z = A, type = "heatmap")
plotly::plot_ly(z = Q, type = "heatmap")
plotly::plot_ly(z = R, type = "heatmap")
plotly::plot_ly(z = A - Q %*% R, type = "heatmap")
plotly::plot_ly(z = t(Q) %*% Q, type = "heatmap")
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

A = np.random.randn(6,6)
Q,R = np.linalg.qr(A)

print(A)
print(Q)
print(R)
print(A - Q @ R)
print(Q.T @ Q)

c = 1.5

plt.imshow(A,vmin=-c,vmax=c,cmap='gray')
plt.show()
plt.imshow(Q,vmin=-c,vmax=c,cmap='gray')
plt.show()
plt.imshow(R,vmin=-c,vmax=c,cmap='gray')
plt.show()
plt.imshow(A - Q @ R,vmin=-c,vmax=c,cmap='gray')
plt.show()
plt.imshow(Q.T @ Q,vmin=-c,vmax=c,cmap='gray')
plt.show()
```

```{python}
import numpy as np

A = np.array([[1, -1]]).T
Q,R = np.linalg.qr(A, 'complete')
Q*np.sqrt(2)
```

## Code Exercises

Exercise 9-1.

```{r}
I <- diag(5)
A <- matrix(data = rnorm(25), nrow = 5)
Q <- qr(A) |> qr.Q()


# QtQ == I
t(Q) %*% Q

# QQt == I
Q %*% t(Q)

# Q^-1Q == I
solve(Q) %*% Q
# matlib::Inverse(Q)

# QQ^-1 == I
Q %*% solve(Q)
```

```{python}
import numpy as np

A = np.random.randn(5,5)
Q,R = np.linalg.qr(A, 'complete')
Q @ np.linalg.inv(Q)
np.linalg.inv(Q) @ Q
```


Exercise 9-2.

```{r}
# parallel decomposition
paradecomp <- function(r,t) {
    return( r %*% ((t(t) %*% r) / (t(r) %*% r)) )
}

# perpendicular decomposition
perpendecomp <- function(r,t) {
    return( t - paradecomp(r,t) )
}
```

```{r}
# get orthogonal vector
orthVec <- function(v1, v2) {
    output <- (t(v1) %*% v2) / (t(v2) %*% v2)
    output <- output * v2
    return(output)
}
```

```{r}
# mat1 <- matrix(data = rnorm(16), nrow = 4)
mat1 <- matrix(data = c(1,2,3,4,5,6,7,8,9), nrow = 3)
qmat <- matrix(nrow = nrow(mat1), ncol = ncol(mat1))

qmat[,1] <- mat1[,1] / norm(mat1[,1], type = "2")

for (j in 2:ncol(mat1)) {
    vec1 <- orthVec(mat1[,(j-1)], mat1[,j])
    normalizer <- mat1[,j] / norm(mat1[,j], type = "2")
    vec1 <- vec1 / normalizer
    qmat[,j] <- vec1
}

qmat
qr(mat1)$qr
```

```{python}
import numpy as np

# create the matrix 
m = 3
n = 3
A = np.random.randn(m,n)
A = np.array([[1,2,3], [4,5,6], [7,8,9]])

# initialize
Q = np.zeros((m,n))


# the GS algo
for i in range(n):
    
    # initialize
    Q[:,i] = A[:,i]
    
    # orthogonalize
    a = A[:,i] # convenience
    for j in range(i): # only to earlier cols
        q = Q[:,j] # convenience
        Q[:,i]=Q[:,i]-np.dot(a,q)/np.dot(q,q)*q
    
    # normalize
    Q[:,i] = Q[:,i] / np.linalg.norm(Q[:,i])

    
# "real" QR decomposition for comparison
Q2,R = np.linalg.qr(A)


# note the possible sign differences.
# seemingly non-zero columns will be 0 when adding
print( np.round( Q-Q2 ,10) ), print(' ')
print( np.round( Q+Q2 ,10) )

```

Exercise 9-3.

```{r}
# Part 1
#create an orthogonal matrix,called U, from the QR decomposition of a 6 × 6 random-numbers matrix
dat <- matrix(data = rnorm(6*6), nrow = 6)
U <- qr(dat) |> qr.Q()
```

```{r}
# Compute the QR decomposition of U, and confirm that R = I
U
qr(U) |> qr.R() |> round(digits = 14)
```

```{r}
mynormF <- function(mat) {
    stopifnot(is.matrix(mat))
    norm = 0
    for (i in 1:nrow(mat)) {
        for (j in 1:ncol(mat)) {
            norm <- norm + mat[i,j]^2
        }
    }
    norm <- sqrt(norm)
    return(norm)
}
```


```{r}
# Part 2

# modify the norms of each column of U
# Set the norms of columns 1–6 to be 10–15

# check norms of each column
for (i in 1:ncol(U)) {
    mynormF(as.matrix(U[,i])) |> print()
}

# multiply scalars for each column to get appropriate norms
U[,1] <- U[,1]*10
U[,2] <- U[,2]*11
U[,3] <- U[,3]*12
U[,4] <- U[,4]*13
U[,5] <- U[,5]*14
U[,6] <- U[,6]*15

# check norms of each column
for (i in 1:ncol(U)) {
    mynormF(as.matrix(U[,i])) |> print()
}
```

```{r}
# Run that modulated U matrix through QR decomposition and confirm that 
# its R is a diagonal matrix with diagonal elements equaling 10–15.

R <- qr(U) |> qr.R() |> round(14)
R

# What is QtQ of this matrix?
t(U) %*% U |> round(13)
```


```{r}
# Part 3

# break the orthogonality of U by setting element U(1,4) = 0. What happens to R and why?

U2 <- U
U2[1,4] <- 0
U2

R <- qr(U2) |> qr.R() |> round(13)
R
```

Exercise 9-4.

```{r}
# copy the code from Exercise 8-2 into a Python function that takes a
# matrix as input and provides its inverse as output.

minormat <- function(mat) {
    outmat <- matrix(data = NA, nrow = nrow(mat), ncol = ncol(mat))
    for (i in 1:nrow(mat)) {
        for (j in 1:ncol(mat)) {
            submat <- mat[-i,-j]
            det <- determinant(submat, logarithm = F)
            det <- det$modulus[1] * det$sign
            outmat[i,j] <- det
        }
    }
    return(outmat)
}

gridmat <- function(mat) {
    outmat <- matrix(data = NA, nrow = nrow(mat), ncol = ncol(mat))
    for (i in 1:nrow(mat)) {
        for (j in 1:ncol(mat)) {
            outmat[i,j] <- (-1)^(i+j)
        }
    }
    return(outmat)
}

cofactorsmat <- function(mat) {
    outmat <- minormat(mat) * gridmat(mat)
    return(outmat)
}

adjugatemat <- function(mat) {
    tcof <- cofactorsmat(mat) |> t()
    det <- determinant(mat, logarithm = F)
    det <- det$modulus[1] * det$sign
    invdet <- 1 / det
    outmat <- invdet * tcof
    return(outmat)
}

oldSchoolInv <- function(mat) {
    outmat <- adjugatemat(mat)
    return(outmat)
}
```

```{r}
# create a 5 × 5 random-numbers matrix.
n = 30
mat1 <- matrix(data = rnorm(n*n), nrow = n)
```

```{r}
# Compute its inverse using the old-school method and the QR decomposition method
# introduced in this chapter.

oldSchoolInv(mat1) %*% mat1
Q <- qr(mat1) |> qr.Q()
t(Q) %*% Q
```

```{r}
# Compute the inverse-estimation error as
# the Euclidean distance from the matrix times its inverse to the true identity matrix
# from np.eye. Make a barplot of the results, showing the two methods on the x-axis
# and the error (Euclidean distance to I) on the y-axis.

old <- sqrt(c(diag(nrow(mat1)) - (oldSchoolInv(mat1) %*% mat1))^2)
new <- sqrt(c(diag(nrow(Q)) - (t(Q) %*% Q))^2)

# old <- dist()
# new <- c(t(Q) %*% Q - diag(5))

data1 <- data.frame(old, new)
data1 <- data1 %>%
    pivot_longer(cols = everything(),
                 names_to = "type",
                 values_to = "error")
```

```{r}
data1 %>%
    ggplot(aes(x = type, y = error)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, linetype = "dashed")
```

```{r}
var.test(data = data1, error ~ type)
t.test(data = data1, error ~ type)
```

Exercise 9-6.

```{r}
# Create an M × M orthogonal matrix as the QR decomposition of a random matrix.


m <- 5
mat1 <- matrix(data = rnorm(m^2), nrow = m)

mat2 <- qr(mat1) |> qr.Q()

# Compute its induced 2-norm and compute its Frobenius norm.
norm(mat2, type = "2")
mynormF(mat2)

# divided by the square root of M. Confirm that both quantities are 1 
norm(mat2, type = "F") / sqrt(m)
```

```{r}
# explore the meaning of the induced norm using matrix-vector multiplication.
# Create a random M-element column vector v

m <- 5
v1 <- matrix(data = rnorm(m), nrow = m)
qv <- mat2 %*% v1

# compute the norms of v and Qv
norm(v1, type = "F")
norm(qv, type = "F")
```

Exercise 9-7.

```{r}
m = 10
n = 4
mat1 <- matrix(data = rnorm(m*n), nrow = m)
```

```{r}
mat1 |> qr() |> qr.R(complete = T)
mat1 |> qr() |> qr.R(complete = F)
R <- mat1 |> qr() |> qr.R(complete = T)
```

```{r}
# the submatrix comprising the first N rows is square and full-rank (when A is full column-rank) and thus
# has a full inverse


R[1:n,]
inv <- R[1:n,] |> qr() |> qr.R()
inv
oldSchoolInv(R[1:n,])
matlib::inv(R[1:n,])
pracma::pinv(R)
```

